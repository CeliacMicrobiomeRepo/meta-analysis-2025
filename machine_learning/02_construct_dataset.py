"""
A script for construction of a dataset suitable for machine learning.

This script takes a predefined analysis group and generates a machine learning-ready
dataset by performing ASV filtering and abundance transformations. It uses data 
generated by '01_phylogseq_to_tsv.R'.

Inputs:
- unfiltered_sample_data.tsv: Metadata for each sample.
- unfiltered_asv_table.tsv: Read counts of ASVs in samples.

The script performs the following steps:
- Reads sample metadata and ASV abundance table for a specified GROUP_NAME.
- Filters ASVs based on minimum abundance and prevalence across datasets.
- Applies a specified transformation to the abundance data (e.g., tss, log10, clr).
- Optionally applies Z-score normalization.
- Saves the final ASV abundance matrix and sample labels as TSV files.
- Generates a detailed 'about.txt' and a 'summary.tsv' for the created dataset.
"""


# SET UP ============================================

# Imports -------------------------------------------
import pandas as pd
import os
from composition_stats import clr, multiplicative_replacement
import numpy as np

# Main options ------------------------------------------
# The analysis group to use 
# Options:     'duodenum_active'   'stool_prospective'   'stool_active'   'stool_treated'
GROUP_NAME = "stool_prospective"   # <-- [!!!] Change this each time
# Transform with  
# Options:     'tss'   'log10'   'log10-sum'   'clr'
TRANFORMATION = 'tss'  
# Apply Z-score to transformed abundances (to each bacteria) 
# Options:     False   True
APPLY_ZSCORE = False             
# Filter before or after transformation?
# Options:     'before'   'after'
WHEN_FILTERING = 'after'    
# Pseudo count
PSEUDO_COUNT = 1e-6

# Inputs ------------------------------------------
# Directory containing input data from 01_phylogseq_to_tsv.R
INPUT_DATA_DIR_PATH = "./machine_learning/all_data/"
# TSV file containing metadata for each sample
SAMPLE_DATA_TSV_PATH = os.path.join(INPUT_DATA_DIR_PATH, GROUP_NAME, "unfiltered_sample_data.tsv")
# TSV file containing relative abundances of ASVs in samples
ASV_TABLE_TSV_PATH = os.path.join(INPUT_DATA_DIR_PATH, GROUP_NAME, "unfiltered_asv_table.tsv")    
# Column containing sample IDs
SAMPLE_ID_COLUMN = "Sample_ID"
# Column containing dataset IDs
DATASET_ID_COLUMN = "Dataset_ID"
# Column to use as labels
# e.g.   'Diagnosed_Celiac'   'Will_Develop_Celiac'
LABEL_COLUMN = "Will_Develop_Celiac"   # <-- [!!!] Change this each time

# Outputs ------------------------------------------
# Construct output directory name based on options
OUTPUT_DIR_NAME = GROUP_NAME + "_" + TRANFORMATION + ("_zscore" if APPLY_ZSCORE else "") + "_" + WHEN_FILTERING
# Output directory path
OUTPUT_DIR_PATH = f"./machine_learning/datasets_main/{OUTPUT_DIR_NAME}/"
# Filtered samples metadata file path
FILTERED_SAMPLES_TSV_OUTPUT_PATH = os.path.join(OUTPUT_DIR_PATH, "sample_labels.tsv")
# ASV abundance matrix file path
ASV_ABUNDANCE_MATRIX_TSV_OUTPUT_PATH = os.path.join(OUTPUT_DIR_PATH, "sample_asv_abundances.tsv")
# About file location
ABOUT_PATH = os.path.join(OUTPUT_DIR_PATH, "about_dataset.txt")

# ASV filter ------------------------------
# Minimum average abundance across a dataset (in X% of datasets) for a taxonomic unit to be included
MIN_AVERAGE_ABUNDANCE_IN_DATASET = 0.0
# Minimum proportion of all samples in a dataset (in X% of datasets) for a taxonomic unit to be included
MIN_PREVALENCE_IN_SAMPLES_IN_DATASET = 0.1
# ...X proportion of the datasets:
IN_N_PROPORTION_DATASETS = 0.0   # When this is 0.0, it means it must be present in at least 1 dataset




def apply_total_sum_scaling(abundances_df):
    """
    Normalise/transform the abundances of an abundance matrix so that: 
        - The all abundances in a sample sum to 1.0

    - Called total sum scaling in this paper: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0160169
    
    (rows are samples, columns are taxonomic units)
    """
    return abundances_df.div(abundances_df.sum(axis=1), axis=0)

def apply_zscore(abundances_df):
    """
    Normalise/transform the abundances of an abundance matrix so that: 
        - The features have a mean of zero and a unit variance across all samples

    - Subtracts the means of features, then divides by SDs of features
    
    (rows are samples, columns are taxonomic units)
    """
    # Calculate the mean and standard deviation for each column (feature)
    feature_means = abundances_df.mean(axis=0)
    feature_stds = abundances_df.std(axis=0, ddof=0)  # ddof=0 for population standard deviation

    # Avoid division by zero for features with zero standard deviation
    feature_stds = feature_stds.replace(0, 1e-6)

    # Normalize the data (z-score transformation)
    normalized_df = (abundances_df - feature_means) / feature_stds

    return normalized_df

def transform_log_sum(abundances_df):
    """
    Apply log10 transformation to the abundances in an abundance matrix.
    1. Log10 transformation to all values (with a small constant to avoid log(0))
    2. Divide all values by the sum of the values in the sample

    Args:
        abundances_df (pd.DataFrame): A DataFrame where each row corresponds 
                                      to a sample and each column corresponds 
                                      to a taxonomic unit's abundance.

    Returns:
        pd.DataFrame: A DataFrame with the same shape as the input, where each 
                      value is log-transformed.
    """
    # Add a small constant to avoid log(0)
    log_transformed_df = abundances_df.apply(lambda col: np.log(col + 1e-6))

    # Scale by the sample total
    log_transformed_df = log_transformed_df.div(log_transformed_df.sum(axis=1), axis=0)
    
    return log_transformed_df

def transform_log_median(abundances_df):
    """
    Apply log10 transformation to the abundances in an abundance matrix.
    1. Log10 transformation to all values (with a small constant to avoid log(0))
    2. From all values, subtract the median of the values in that sample

    Args:
        abundances_df (pd.DataFrame): A DataFrame where each row corresponds 
                                      to a sample and each column corresponds 
                                      to a taxonomic unit's abundance.

    Returns:
        pd.DataFrame: A DataFrame with the same shape as the input, where each 
                      value is log-transformed and centred on the median of each sample.
    """
    # Add a small constant to avoid log(0)
    log_transformed_df = abundances_df.apply(lambda col: np.log(col + 1e-6))

    # Centre samples on their median
    log_transformed_df = log_transformed_df.sub(log_transformed_df.median(axis=1), axis=0)
    
    return log_transformed_df

def transform_clr(abundances_df):
    """
    Apply the Centered Log-Ratio (CLR) transformation to the abundances in an abundance matrix.

    The transformation is applied to each row of the input DataFrame, which 
    represents the abundances of different taxonomic units in a sample.

    Prior to transformation, zero abundances are replaced with a small value using multiplicative_replacement.
    """

    # Replace zero abundances with a small value using multiplicative_replacement
    abundances_array = multiplicative_replacement(abundances_df.values, PSEUDO_COUNT)

    # Convert back to DataFrame
    abundances_df = pd.DataFrame(abundances_array, index=abundances_df.index, columns=abundances_df.columns)

    # Apply the clr transformation to each row
    transformed_df = abundances_df.apply(lambda row: pd.Series(clr(row.values), index=abundances_df.columns), axis=1)
    
    return transformed_df



# MAIN ================================================
# Create output directory if it doesn't exist
if not os.path.exists(OUTPUT_DIR_PATH):
    os.makedirs(OUTPUT_DIR_PATH)



# Read data ------------------------------------
# This should be read counts

# Read samples metadata
samples_df = pd.read_csv(SAMPLE_DATA_TSV_PATH, sep="\t")
# Read asv table
abundance_table_df = pd.read_csv(ASV_TABLE_TSV_PATH, sep="\t")
# Set sample IDs as index
asv_abundance_matrix = abundance_table_df.set_index(SAMPLE_ID_COLUMN)

initial_sample_count = len(samples_df)
initial_asv_count = len(asv_abundance_matrix.columns)

print(f"\nNumber of samples: {initial_sample_count}")
print(f"Initial number of ASVs: {initial_asv_count}")



# Determine which ASVs should be filtered -----------------------------------------
# This simply decides which ASVs to remove based on abundances and prevalences
# Actual removal is done before/after transformation depending on FILTER_BEFORE_TRANSFORMATION
tss_asv_abundance_matrix = apply_total_sum_scaling(asv_abundance_matrix.copy())

# Calculate the threshold number of datasets
num_datasets = len(samples_df[DATASET_ID_COLUMN].unique())
dataset_threshold = max(round(IN_N_PROPORTION_DATASETS * num_datasets), 1)
print(f"Filtering {len(asv_abundance_matrix.columns)} ASVs that do not meet the abundance and prevalence conditions in at least {IN_N_PROPORTION_DATASETS * 100}% of datasets.")
print(f"This corresponds to a minimum of {dataset_threshold} datasets out of {num_datasets}.")

# Calculate the average abundance for each ASV in each dataset
dataset_avg_abundances = tss_asv_abundance_matrix.reset_index().merge(
    samples_df[[SAMPLE_ID_COLUMN, DATASET_ID_COLUMN]], 
    on=SAMPLE_ID_COLUMN
).drop(SAMPLE_ID_COLUMN, axis=1).groupby(DATASET_ID_COLUMN).mean()

# Get indices of ASVs to remove based on abundance
asvs_meeting_abundance_threshold = (dataset_avg_abundances >= MIN_AVERAGE_ABUNDANCE_IN_DATASET).sum() >= dataset_threshold
asvs_to_remove_abundance = dataset_avg_abundances.columns[~asvs_meeting_abundance_threshold].tolist()

# Print number of ASVs to remove from abundance filter
print(f"Number of ASVs to remove based on abundance threshold: {len(asvs_to_remove_abundance)}")

# Calculate the prevalence for each ASV in each dataset
dataset_prevalences = tss_asv_abundance_matrix.reset_index().merge(
    samples_df[[SAMPLE_ID_COLUMN, DATASET_ID_COLUMN]], 
    on=SAMPLE_ID_COLUMN
).drop(SAMPLE_ID_COLUMN, axis=1).groupby(DATASET_ID_COLUMN).apply(
    lambda x: (x > 0).sum() / len(x), 
    include_groups=False  # Add this parameter to address the warning
)

# Get indices of ASVs to remove based on prevalence
asvs_meeting_prevalence_threshold = (dataset_prevalences >= MIN_PREVALENCE_IN_SAMPLES_IN_DATASET).sum() >= dataset_threshold
asvs_to_remove_prevalence = dataset_prevalences.columns[~asvs_meeting_prevalence_threshold].tolist()

# Print number of ASVs to remove from prevalence filter
print(f"Number of ASVs to remove based on prevalence threshold: {len(asvs_to_remove_prevalence)}")

# Print the number of ASVs to be removed from both (excluding duplicates)
all_asvs_to_remove = list(set(asvs_to_remove_abundance + asvs_to_remove_prevalence))
if 'Dataset_ID' in all_asvs_to_remove:
    all_asvs_to_remove.remove('Dataset_ID')
print(f"Number of ASVs to remove based on abundance OR prevalence threshold: {len(all_asvs_to_remove)}")
print(f"...Leaving {len(asv_abundance_matrix.columns) - len(all_asvs_to_remove)} ASVs in the dataset.")




# Filter ASVs before transformation -----------------------------------------
# This removes ASVs, the data is kept as untransformed read counts

if WHEN_FILTERING == 'before': 

    # Remove ASVs
    asv_abundance_matrix = asv_abundance_matrix.drop(columns=all_asvs_to_remove)


# Normalisation and transformation ------------------------------------------
# These are applied to raw read counts

# Transformation
if TRANFORMATION == 'tss':
    asv_abundance_matrix = apply_total_sum_scaling(asv_abundance_matrix)

elif TRANFORMATION == 'log10':
    asv_abundance_matrix = transform_log_median(asv_abundance_matrix)

elif TRANFORMATION == 'log10-sum':
    asv_abundance_matrix = transform_log_sum(asv_abundance_matrix)

elif TRANFORMATION == 'clr':
    asv_abundance_matrix = transform_clr(asv_abundance_matrix)


# Z-score bacteria
if APPLY_ZSCORE:
    asv_abundance_matrix = apply_zscore(asv_abundance_matrix)



# Filter ASVs after transformation -----------------------------------------
# This removes ASVs, the data is kept as it is

if WHEN_FILTERING == 'after': 

    # Remove ASVs
    asv_abundance_matrix = asv_abundance_matrix.drop(columns=all_asvs_to_remove)




# Summary of labels ------------------------------------------

# Print the number of samples with each label in each dataset
print("\nNumber of samples with each label in each dataset:")
for dataset_id, group in samples_df.groupby(DATASET_ID_COLUMN):
    print(dataset_id + ":")
    label_counts = group[LABEL_COLUMN].value_counts()
    for label, count in label_counts.items():
        print(f"  {label}: {count}")

# Print the number of samples with each label overall
print("\nTotal:")
overall_label_counts = samples_df[LABEL_COLUMN].value_counts()
for label, count in overall_label_counts.items():
    print(f"  {label}: {count}")

print()


# Write dataset to file ------------------------------------------
# Features
asv_abundance_matrix.to_csv(ASV_ABUNDANCE_MATRIX_TSV_OUTPUT_PATH, sep="\t")
# Replace the main label in samples_df with "Disease_Status" for consistency with following scripts
samples_df = samples_df.rename(columns={LABEL_COLUMN: "Disease_Status"})
# Labels
samples_df.to_csv(FILTERED_SAMPLES_TSV_OUTPUT_PATH, sep="\t", index=False)

# Write about.txt
with open(ABOUT_PATH, "w") as about_file:
    about_file.write("Dataset Construction Summary\n")
    about_file.write("Input Files:\n")
    about_file.write(f"  - Sample Data: {SAMPLE_DATA_TSV_PATH}\n")
    about_file.write(f"  - ASV Table: {ASV_TABLE_TSV_PATH}\n\n")
    about_file.write("Output Files:\n")
    about_file.write(f"  - Filtered Samples Metadata: {FILTERED_SAMPLES_TSV_OUTPUT_PATH}\n")
    about_file.write(f"  - ASV Abundance Matrix: {ASV_ABUNDANCE_MATRIX_TSV_OUTPUT_PATH}\n\n")
    about_file.write("Transformation Options:\n")
    about_file.write(f"  - Pseudo count: {PSEUDO_COUNT}\n")
    about_file.write(f"  - Transformation: {TRANFORMATION}\n")
    about_file.write(f"  - Z-score: {APPLY_ZSCORE}\n")
    about_file.write(f"  - Filtering: {WHEN_FILTERING}\n\n")
    about_file.write("ASV Filter:\n")
    about_file.write(f"  - Minimum average abundance across a dataset (in X% of datasets) for a taxonomic unit to be included: {MIN_AVERAGE_ABUNDANCE_IN_DATASET}\n")
    about_file.write(f"  - Minimum proportion of all samples in a dataset (in X% of datasets) for a taxonomic unit to be included: {MIN_PREVALENCE_IN_SAMPLES_IN_DATASET}\n")
    about_file.write(f"  - ...X proportion of the datasets: {IN_N_PROPORTION_DATASETS}\n\n")
    about_file.write("Summary:\n")
    about_file.write(f"  - Initial number of samples: {initial_sample_count}\n")
    about_file.write(f"  - Final number of samples: {len(samples_df)}\n")
    about_file.write(f"  - Initial number of ASVs: {initial_asv_count}\n")
    about_file.write(f"  - Final number of ASVs: {len(asv_abundance_matrix.columns)}\n")
    about_file.write(f"  - ASVs removed: {len(all_asvs_to_remove)}\n\n")
    about_file.write("Sample Label Details:\n")
    about_file.write("Overall label distribution:\n")
    for label, count in overall_label_counts.items():
        about_file.write(f"  - {label}: {count}\n")
    about_file.write("\nLabel distribution by dataset:\n")
    for dataset_id, group in samples_df.groupby(DATASET_ID_COLUMN):
        about_file.write(f"  Dataset {dataset_id}:\n")
        label_counts = group["Disease_Status"].value_counts()
        for label, count in label_counts.items():
            about_file.write(f"    - {label}: {count}\n")

# Summary TSV
summary_file_path = os.path.join(OUTPUT_DIR_PATH, "summary.tsv")
summary_data = {
    "Dataset Name": [os.path.basename(OUTPUT_DIR_PATH.rstrip('/'))],
    "Number of Datasets Included": [samples_df[DATASET_ID_COLUMN].nunique()],
    "Positive Samples Included": [overall_label_counts.get(True, 0)],
    "Negative Samples Included": [overall_label_counts.get(False, 0)],
    "Total Included Samples": [len(samples_df)],
    "ASVs Included": [len(asv_abundance_matrix.columns)],
    "ASVs Excluded": [len(all_asvs_to_remove)]
}
summary_df = pd.DataFrame(summary_data)
summary_df.to_csv(summary_file_path, sep="\t", index=False)



print("Done!")
